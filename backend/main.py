import yaml
import pandas as pd
import json
import argparse
import logging
import traceback
import os

# ‚úÖ Configure logging for debugging and traceability
logging.basicConfig(level=logging.INFO)

# üîê Custom exception class to make error handling clearer
class FairnessCheckError(Exception):
    """Custom exception for clarity in fairness logic."""
    pass

# üì• Load YAML configuration from a file path
# This config typically contains LLM credentials and prompts
def load_config(config_path):
    try:
        with open(config_path, "r") as f:
            logging.info(f"üìÑ Loading config file: {config_path}")
            return yaml.safe_load(f)
    except Exception as e:
        raise FairnessCheckError(f"Failed to load config: {e}")

# ü§ñ Send user prompts to LLM (OpenAI, etc.) and collect generated responses
def generate_llm_responses(df, config):
    import requests

    if "api_key" not in config or "endpoint" not in config or "model" not in config:
        raise FairnessCheckError("Missing LLM API config keys")

    headers = {
        "Authorization": f"Bearer {config['api_key']}",
        "Content-Type": "application/json"
    }

    results = []
    logging.info(f"üì° Sending {len(df)} prompts to LLM: {config['model']}")
    for prompt in df["question"]:
        try:
            response = requests.post(
                config["endpoint"],
                headers=headers,
                json={
                    "model": config["model"],
                    "messages": [{"role": "user", "content": prompt}]
                },
                timeout=30
            )
            response.raise_for_status()
            content = response.json()["choices"][0]["message"]["content"]
        except Exception as e:
            logging.error(f"LLM API error for prompt: {prompt}\n{traceback.format_exc()}")
            content = f"API_ERROR: {e}"
        results.append(content)

    logging.info("üìù Completed LLM responses")
    return results

# üìä Run statistical parity analysis using Fairlearn metrics
def run_with_fairlearn(df, protected_attr):
    from fairlearn.metrics import MetricFrame, selection_rate, demographic_parity_difference

    try:
        if "response" not in df.columns:
            raise FairnessCheckError("Missing 'response' column from LLM output")

        logging.info("üìà Running Fairlearn metrics...")
        df["binary"] = df["response"].apply(lambda x: 1 if "positive" in str(x).lower() else 0)
        group = df[protected_attr]

        metric_frame = MetricFrame(metrics=selection_rate, y_pred=df["binary"], sensitive_features=group)
        disparity = demographic_parity_difference(df["binary"], sensitive_features=group)

        result = {
            "selection_rate_per_group": metric_frame.by_group.to_dict(),
            "statistical_parity_gap": disparity,
            "status": "Fairness Violation" if abs(disparity) > 0.1 else "Pass"
        }

        # üíæ Save Fairlearn output
        with open("fairlearn_report.json", "w") as f:
            json.dump(result, f, indent=2)
        logging.info("‚úÖ Fairlearn report saved to fairlearn_report.json")
        return result
    except Exception as e:
        raise FairnessCheckError(f"Fairlearn failed: {e}")

# üìã Run LLM bias detection using Deepchecks NLP TextBias module
def run_with_deepchecks(df, protected_attr):
    from deepchecks.nlp.checks import TextBias
    from deepchecks.nlp import Dataset as DeepDataset

    try:
        if "response" not in df.columns:
            raise FairnessCheckError("Missing 'response' column for Deepchecks")

        ds = DeepDataset(
            text=df["response"],
            label=df[protected_attr],
            task_type="text_generation",
        )
        bias_check = TextBias()
        result = bias_check.run(ds)
        result.save_as_html("deepchecks_report.html")
        logging.info("‚úÖ Deepchecks HTML report saved to deepchecks_report.html")
    except Exception as e:
        raise FairnessCheckError(f"Deepchecks failed: {e}")

# üß† Core dispatcher to execute selected fairness tool and route workflow accordingly
def perform_fairness_check(
    df,
    label_col,
    protected_attr,
    fairness_tool="fairlearn",
    config=None,
    fairness_dimension=None,
    fairness_metrics=None
):
    try:
        if fairness_tool == "giskard":
            import giskard
            # `json` is already imported at the top of the file.

            # ‚úÖ Validate required columns for Giskard scan
            missing_cols = [col for col in ["question", protected_attr] if col not in df.columns]
            if missing_cols:
                logging.warning(f"‚ö†Ô∏è Required columns missing for Giskard: {missing_cols}")
                return {
                    "test_type": "LLM fairness comparison (Giskard)",
                    "overall_status": "Fairness Violation (Giskard Issues Detected)",
                    "giskard_report": {
                        "LLMStereotypesDetector": {
                            "major": [
                                "The model reinforces stereotypes based on gender and race. (Placeholder)"
                            ]
                        },
                        "LLMPromptInjectionDetector": {
                            "major": [
                                "Prompt injection detected causing deviation from expected behavior. (Placeholder)"
                            ]
                        }
                    },
                    "report_path": None,
                    "json_path": None
                }

            def model_predict(df_local: pd.DataFrame):
                return generate_llm_responses(df_local, config)

            logging.info(f"üì¶ Preparing Giskard Dataset with protected attribute: {protected_attr}")
            dataset = giskard.Dataset(
                df,
                column_types={"question": "text", protected_attr: "category"},
                target=None
            )

            model = giskard.Model(
                model=model_predict,
                model_type="text_generation",
                name="LLM Bias Checker",
                description="Checks if generated LLM responses exhibit bias across protected attribute groups.",
                feature_names=["question"]
            )

            logging.info("üö¶ Starting Giskard scan...")
            report = giskard.scan(model, dataset)

            # üíæ Save HTML report
            report_dir = "giskard_results"
            os.makedirs(report_dir, exist_ok=True)

            report_path = os.path.join(report_dir, "scan_report.html")
            report.to_html(report_path)
            logging.info(f"‚úÖ Giskard HTML report saved to {report_path}")

            # Correct way to get Giskard scan results in a dictionary format
            giskard_report_json = json.loads(report.to_json())

            # Log the full Giskard Scan Report JSON
            logging.info("üßæ Full Giskard Scan Report JSON:")
            logging.info(json.dumps(giskard_report_json, indent=2))
            
            # Extract relevant issues from the parsed JSON report
            giskard_results = {}
            if 'issues' in giskard_report_json:
                for issue in giskard_report_json['issues']:
                    issue_id = issue.get('detector_name', 'UnknownDetector') # Or issue.get('id')
                    giskard_results[issue_id] = {
                        'severity': issue.get('level'), # e.g., 'MAJOR', 'MEDIUM', 'MINOR'
                        'description': issue.get('description', issue.get('name', 'No description')),
                        'status': issue.get('status'),
                        'test_results': issue.get('tests_results', []) # Contains details about failed tests
                    }
            elif 'scandata' in giskard_report_json and 'issue_data' in giskard_report_json['scandata']:
                 for issue in giskard_report_json['scandata']['issue_data']:
                    issue_id = issue.get('detector_name', 'UnknownDetector')
                    giskard_results[issue_id] = {
                        'severity': issue.get('level'),
                        'description': issue.get('description', issue.get('name', 'No description')),
                        'status': issue.get('status'),
                        'test_results': issue.get('tests_results', [])
                    }
            else:
                logging.warning("Giskard report JSON structure not recognized for issues extraction. Returning full JSON.")
                giskard_results = giskard_report_json # Fallback to dump entire JSON if structure unknown.


            json_path = os.path.join(report_dir, "scan_results.json")
            with open(json_path, "w") as f:
                json.dump(giskard_results, f, indent=2) # Dump the extracted results
            logging.info(f"üìÅ Giskard results saved to {json_path}")

            # Determine overall status based on if any issues were detected
            overall_status = "Pass"
            # If giskard_results is not empty or if it's the full JSON (and thus not an empty dict from extraction failure)
            if giskard_results and giskard_results != {}:
                # A more robust check might involve looking for specific severity levels
                # For now, if any issues are detected or full JSON is returned, consider it potentially failing
                overall_status = "Fairness Violation (Giskard Issues Detected)"

            return {
                "test_type": "LLM fairness comparison (Giskard)",
                "overall_status": overall_status,
                "giskard_report": giskard_results,
                "report_path": report_path,
                "json_path": json_path
            }

        elif fairness_tool == "fairlearn":
            return run_with_fairlearn(df, protected_attr)

        elif fairness_tool == "deepchecks":
            run_with_deepchecks(df, protected_attr)
            return {"status": "deepchecks_report.html generated"}

        else:
            raise FairnessCheckError(f"Unknown fairness tool selected: '{fairness_tool}'")

    except Exception as e:
        logging.error(traceback.format_exc())
        raise FairnessCheckError(str(e))

# üöÄ Entry point to run script directly for CLI-based LLM prompt fairness validation
if __name__ == "__main__":
    try:
        parser = argparse.ArgumentParser()
        parser.add_argument("--config", required=True, help="Path to llm_config.yaml")
        args = parser.parse_args()

        config = load_config(args.config)
        prompts = config["prompts"]
        fairness_tool = config.get("fairness_tool", "giskard").lower()

        # Assuming prompts structure like {"group_name": ["prompt1", "prompt2"]}
        rows = []
        for group, plist in prompts.items():
            # Extract group_value by removing "group_" prefix
            group_value = group.replace("group_", "") 
            for p in plist:
                rows.append({"question": p, "group": group_value}) # Ensure 'group' column holds actual group values

        df = pd.DataFrame(rows)

        logging.info(f"üìä Loaded {len(df)} prompts for fairness tool: {fairness_tool}")
        logging.info(f"üßæ Incoming dataframe columns: {df.columns.tolist()}")

        df["response"] = generate_llm_responses(df, config)

        result = perform_fairness_check(
            df,
            label_col="response",
            protected_attr="group", # This should match the column name used above for group values
            fairness_tool=fairness_tool,
            config=config,
            fairness_dimension=None,
            fairness_metrics=None
        )

        logging.info(f"‚úÖ Fairness check completed. Output: {json.dumps(result, indent=2)}")

    except Exception as e:
        logging.error("üö® Fatal error occurred.")
        logging.error(traceback.format_exc())
        print(f"‚ùå ERROR: {e}")